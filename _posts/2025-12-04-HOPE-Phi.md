---
layout: post
title: "HOPE-Phi: Inyección de Sistemas de Memoria Continua en SLMs mediante Apilamiento de Niveles Ad-Hoc"
date: 2025-12-04 10:00:00 -0000
author: Z43L
tags: [machine-learning, nlp, slm, nested-learning, phi-3, ia]
reading_time: 8
excerpt: "Metodología de 'Cirugía Arquitectónica' para transformar un modelo denso estándar (Phi-3) en un módulo de aprendizaje HOPE, permitiendo especialización en razonamiento matemático."
---

# HOPE-Phi: Inyección de Sistemas de Memoria Continua en SLMs mediante Apilamiento de Niveles Ad-Hoc

**Autor:** Z43L  
**Fecha:** 4 de diciembre de 2025



## Resumen
Los Modelos de Lenguaje Pequeños (SLMs) suelen estar limitados por su capacidad estática tras el pre-entrenamiento, lo que dificulta su adaptación a dominios de alta logicidad como la generación de código sin un re-entrenamiento costoso. Basándonos en el paradigma de *Nested Learning* (NL), proponemos una metodología de "Cirugía Arquitectónica" para transformar un modelo denso estándar (Phi-3) en un módulo de aprendizaje HOPE. Introducimos el uso de *Continuum Memory Systems* (CMS) inicializados mediante *Ad-hoc Level Stacking*, permitiendo la especialización del modelo en tareas de razonamiento matemático en menos de 8 horas de cómputo en una sola estación de trabajo (2x RTX 6000). Nuestros resultados preliminares demuestran la viabilidad de bifurcar la memoria del modelo en ramas de frecuencia rápida y lenta para capturar dependencias sintácticas y semánticas simultáneamente.

## 1. Introducción
El entrenamiento de Modelos de Lenguaje Grandes (LLMs) desde cero es prohibitivo para la mayoría de los laboratorios de investigación. Sin embargo, los modelos actuales sufren de lo que Behrouz et al. denominan la "ilusión de la arquitectura de Deep Learning": son sistemas estáticos que no aprenden durante la inferencia y cuyo conocimiento se congela tras el pre-entrenamiento.

En este trabajo, exploramos la aplicación del paradigma *Nested Learning* (NL) para dotar a un modelo de la clase 3B (Phi-3-mini) de capacidades de adaptación dinámica. Específicamente, nos centramos en la implementación eficiente del Sistema de Memoria Continua (CMS), reemplazando los perceptrones multicapa (MLP) estáticos por bloques dinámicos de doble rama.

**Nuestra contribución es triple:**
* Una implementación práctica de *Ad-hoc Level Stacking* para inicializar arquitecturas complejas desde pesos pre-entrenados.
* Un protocolo de entrenamiento optimizado para hardware de estación de trabajo (192GB VRAM) con restricciones de tiempo estrictas (8 horas).
* La liberación del código de adaptación para la comunidad open-source.

## 2. Metodología

### 2.1. Transformación Arquitectónica HOPE
Partimos del modelo base `Phi-3-mini-4k-instruct`. Identificamos que los bloques MLP tradicionales actúan como almacenes de conocimiento estático. Siguiendo la teoría de NL, sustituimos estos bloques por módulos CMS definidos como:

$$
y_t = \sigma(W_{gate} \cdot x_t) \odot \text{MLP}_{fast}(x_t) + (1 - \sigma(W_{gate} \cdot x_t)) \odot \text{MLP}_{slow}(x_t)
$$

Donde $\text{MLP}_{slow}$ preserva el conocimiento sintáctico general y $\text{MLP}_{fast}$ se adapta rápidamente al contexto local (definiciones de variables, lógica de problemas específicos).

### 2.2. Inicialización Ad-Hoc
En lugar de inicializar los nuevos parámetros aleatoriamente, aplicamos la técnica de *Ad-hoc Level Stacking*:

1.  Congelamos parcialmente la rama lenta ($\text{MLP}_{slow} \leftarrow \theta_{pre}$).
2.  Clonamos los pesos para la rama rápida ($\text{MLP}_{fast} \leftarrow \theta_{pre}$).
3.  Inicializamos la compuerta de mezcla ($W_{gate}$) para favorecer ligeramente la estabilidad inicial.

Esta estrategia elimina la necesidad de un pre-entrenamiento masivo ("Cold Start"), permitiendo que el modelo sea funcional desde el paso 0.

## 3. Configuración Experimental

### 3.1. Hardware y Entorno
Los experimentos se realizaron en un nodo de computación equipado con:
* **GPU:** 2x NVIDIA RTX 6000 Ada Generation (96GB VRAM cada una, 192GB total).
* **Precisión:** `bfloat16` nativo para maximizar el *throughput* en núcleos Tensor.
* **Optimizador:** AdamW fusionado con decaimiento de peso de 0.01.

### 3.2. Dataset y Tarea
Para forzar la capacidad de razonamiento lógico, utilizamos el dataset `microsoft/orca-math-word-problems-200k`. Este conjunto de datos requiere cadenas de pensamiento (*Chain-of-Thought*) explícitas, ideales para probar la capacidad del CMS de mantener contextos lógicos a largo plazo.

### 3.3. Protocolo de Entrenamiento Rápido
Dada la restricción de 8 horas, configuramos el entrenamiento para maximizar la densidad de tokens:
* **Longitud de Secuencia:** 4096 tokens (contexto completo).
* **Batch Size Efectivo:** 32 (8 por GPU × 4 pasos de acumulación).
* **Mecanismo de Atención:** SDPA (*Scaled Dot Product Attention*) nativo de PyTorch para evitar overhead de compilación.

## 4. Resultados Experimentales

### 4.1. Dinámica de Convergencia
El entrenamiento se ejecutó durante 250 pasos de optimización. Como se muestra en la Tabla 1, la inyección del módulo CMS demostró una adaptación rápida. La pérdida inicial de 14.80 (típica de un modelo recién modificado) disminuyó drásticamente a 1.74 en menos de 2.5 horas, indicando que el mecanismo de *Ad-hoc Level Stacking* permitió a la "rama rápida" del CMS especializarse en la lógica matemática sin destruir el conocimiento lingüístico base.

| Época | Pérdida (Loss) | Grad Norm |
| :--- | :--- | :--- |
| 0.01 | 14.8072 | 295.37 |
| 0.05 | 7.4433 | 126.50 |
| 0.10 | 4.0388 | 46.79 |
| 0.15 | 2.7858 | 36.76 |
| 0.20 | 2.0833 | 32.24 |
| 0.28 | 1.7395 | 29.42 |

*Cuadro 1: Progresión de la pérdida durante la fase de adaptación del CMS.*

### 4.2. Eficiencia Computacional
Utilizando 2 GPUs RTX 6000, logramos una velocidad de iteración estable de 41.12 segundos/iteración con un tamaño de lote efectivo de 32 y secuencias de 4k. Esto se traduce en un *throughput* aproximado de:

$$
\frac{32 \times 4096 \text{ tokens}}{41.12 \text{ s}} \approx 3,188 \text{ tokens/s}
$$

Este rendimiento confirma que la arquitectura CMS, al estar basada en operaciones matriciales densas, mantiene una eficiencia comparable a los MLPs estándar, permitiendo el procesamiento de $\approx 2.8$ billones de tokens en una jornada de trabajo estándar (24h), o una adaptación efectiva (300M tokens) en la ventana de 3 horas observada.

## 5. Conclusión y Trabajo Futuro
Hemos demostrado que es posible modernizar arquitecturas de LLMs existentes bajo el paradigma *Nested Learning* utilizando recursos de hardware accesibles. El modelo resultante, **HOPE-Phi**, presenta una estructura teóricamente superior para tareas de código y razonamiento, logrando una convergencia de pérdida de 1.74 en tiempo récord.

El trabajo futuro se centrará en:
1.  Implementar el optimizador M3 (*Multi-scale Momentum Muon*) para mejorar la convergencia final.
2.  Evaluar el rendimiento en benchmarks de recuperación de contexto largo (*"Needle in a Haystack"*).